ğŸ“ Text Summarization using DistilBART (Hugging Face)

This project implements abstractive text summarization using the pre-trained DistilBART CNN model from Hugging Face Transformers.

It takes a long input text and generates a short, meaningful, and coherent summary.

ğŸ” Model Used

Model Name: sshleifer/distilbart-cnn-12-6

Architecture: Transformer (Encoderâ€“Decoder)

Task: Abstractive Text Summarization

Training Dataset: CNN / DailyMail

Framework: PyTorch

ğŸ“¦ Requirements

Make sure Python 3.8+ is installed on your system.

Install required dependencies:
pip install torch transformers

(Optional but Recommended)
pip install sentencepiece

ğŸ“ Project Structure
.
â”œâ”€â”€ summarizer.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt   # optional

âš™ï¸ How the Code Works (High Level)

Loads a pre-trained tokenizer and summarization model

Converts the input text into tokenized tensors

Uses beam search decoding to generate a high-quality summary

Decodes the generated tokens into human-readable text